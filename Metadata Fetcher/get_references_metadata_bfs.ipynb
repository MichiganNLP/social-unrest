{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import bibtexparser\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "current_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_metadata_uid_using_doi(doi, api_key):\n",
    "    \n",
    "    \"\"\"\n",
    "    Fetches metadata and UID for a given DOI using the Web of Science API.\n",
    "\n",
    "    Parameters:\n",
    "    doi (str): The DOI of the paper to fetch metadata for.\n",
    "    api_key (str): The API key for authenticating with the Web of Science API.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the metadata JSON response and the UID string.\n",
    "    \"\"\"\n",
    "\n",
    "    url = f\"https://api.clarivate.com/api/wos?databaseId=WOK&usrQuery=DO=({doi})\"\n",
    "    headers = {\n",
    "        'X-ApiKey': api_key\n",
    "    }\n",
    "    # print(\"Current time from DOI:\", datetime.now())\n",
    "    response = requests.get(url, headers=headers)\n",
    "    time.sleep(0.5)\n",
    "    if response.status_code == 200:\n",
    "        # print(f'\\nReceiving response for DOI {doi}:\\n{response.json()}')\n",
    "        uid = ''\n",
    "        if response.json()['QueryResult']['RecordsFound'] > 0:\n",
    "            uid = response.json()['Data']['Records']['records']['REC'][0]['UID']\n",
    "            # print('UID : ', uid.replace(':', '%3A'))\n",
    "        return response.json(), uid.replace(':', '%3A').replace('(', '%28').replace(')', '%29')\n",
    "\n",
    "    else:\n",
    "        print(f\"\\nError fetching metadata for DOI {doi}: {response.status_code}\")\n",
    "        print(\"Response Content:\", response.content)\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_references_using_uid(uid, api_key):\n",
    "\n",
    "    \"\"\"\n",
    "    Fetches all references for a given UID using the Web of Science API, handling pagination.\n",
    "\n",
    "    Parameters:\n",
    "    uid (str): The UID of the paper to fetch references for.\n",
    "    api_key (str): The API key for authenticating with the Web of Science API.\n",
    "\n",
    "    Returns:\n",
    "    Dictionary: A dictionary of references. \n",
    "    List of references can be accessed using the 'Data' key in the Dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    all_references = {'Data':[], 'QueryResult':{}}\n",
    "    first_index = 1\n",
    "    count = 100\n",
    "\n",
    "    while True:\n",
    "        url = f\"https://api.clarivate.com/api/wos/references?databaseId=WOK&uniqueId={uid}&count={count}&firstRecord={first_index}\"\n",
    "        headers = {\n",
    "            'X-ApiKey': api_key\n",
    "        }\n",
    "        # print(\"Current time from Reference:\", datetime.now())\n",
    "        response = requests.get(url, headers=headers)\n",
    "        time.sleep(0.5)\n",
    "        if response.status_code == 200:\n",
    "            references = response.json()\n",
    "            all_references['Data'].extend(references['Data'])\n",
    "            all_references['QueryResult'] = references['QueryResult']\n",
    "            records_found = references.get('QueryResult', {}).get('RecordsFound', 0)\n",
    "            if first_index + count > records_found:\n",
    "                break\n",
    "            first_index += count\n",
    "        else:\n",
    "            print(f\"\\nError fetching references for UID {uid}: {response.status_code}\")\n",
    "            print(\"Response Content:\", response.content)\n",
    "            break\n",
    "    # print(f'\\nReceiving references for UID {uid}:\\n{all_references}\\n')\n",
    "    return all_references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dois(references):\n",
    "\n",
    "    \"\"\"\n",
    "    Extracts DOIs from a list of references.\n",
    "\n",
    "    Parameters:\n",
    "    references (list): A list of reference dictionaries.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of DOIs extracted from the references.\n",
    "    \"\"\"\n",
    "\n",
    "    dois = []\n",
    "    for ref in references['Data']:\n",
    "        doi = ref.get('DOI')\n",
    "        if doi:\n",
    "            doi = doi.replace('/', '%2F').replace('.', '%2E').replace('-', '%2D').replace('(', '%28').replace(')', '%29')\n",
    "            dois.append(doi)\n",
    "    return dois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relevant_metadata(metadata):\n",
    "    \"\"\"\n",
    "    Extracts only the relevant data from the metadata.\n",
    "    Relevant data includes:\n",
    "        - 'wos_id': '',\n",
    "        - 'doi': '',\n",
    "        - 'title': '',\n",
    "        - 'authors': [],\n",
    "        - 'abstract': '',\n",
    "        - 'keywords': [],\n",
    "        - 'document_type': [],\n",
    "        - 'publisher': '',\n",
    "        - 'publication_year': '',\n",
    "        - 'publication_date': ''\n",
    "\n",
    "    Parameters:\n",
    "    metadata (dict): All metadata for a specific article obtained from WoS.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary of all the relevant metadata extracted from the article.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        record = metadata['Data']['Records']['records']['REC'][0]\n",
    "        wos_id = record.get('UID', '')\n",
    "\n",
    "        # Extract DOI from identifiers with a try-except block for safety\n",
    "        doi = ''\n",
    "        try:\n",
    "            identifiers = record.get('dynamic_data', {}).get('cluster_related', {}).get('identifiers', {}).get('identifier', [])\n",
    "            if isinstance(identifiers, list):\n",
    "                doi = next((id_info['value'] for id_info in identifiers if isinstance(id_info, dict) and id_info.get('type') == 'doi'), '')\n",
    "            elif isinstance(identifiers, dict) and identifiers.get('type') == 'doi':\n",
    "                doi = identifiers.get('value', '')\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting DOI: {e}\")\n",
    "        \n",
    "        # Extract title\n",
    "        title = next((title_info['content'] for title_info in record.get('static_data', {}).get('summary', {}).get('titles', {}).get('title', []) if title_info['type'] == 'item'), '')\n",
    "\n",
    "        # Extract authors\n",
    "        authors = []\n",
    "        try:\n",
    "            names_data = record.get('static_data', {}).get('summary', {}).get('names', {}).get('name', [])\n",
    "            if isinstance(names_data, list):\n",
    "                authors = [author['full_name'] for author in names_data if author['role'] == 'author']\n",
    "            elif isinstance(names_data, dict) and names_data.get('role') == 'author':\n",
    "                authors = [names_data.get('full_name', '')]\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting authors: {e}\")\n",
    "\n",
    "        # Extract abstract\n",
    "        abstract = ''\n",
    "        try:\n",
    "            abstract_data = record.get('static_data', {}).get('fullrecord_metadata', {}).get('abstracts', {}).get('abstract', {})\n",
    "            if isinstance(abstract_data, dict):\n",
    "                abstract = abstract_data.get('abstract_text', {}).get('p', '')\n",
    "            elif isinstance(abstract_data, list):\n",
    "                for abs_item in abstract_data:\n",
    "                    if isinstance(abs_item, dict):\n",
    "                        abstract = abs_item.get('abstract_text', {}).get('p', '')\n",
    "                        if abstract:\n",
    "                            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting abstract: {e}\")\n",
    "\n",
    "        # Extract keywords\n",
    "        keywords = record.get('static_data', {}).get('fullrecord_metadata', {}).get('keywords', {}).get('keyword', [])\n",
    "        \n",
    "        # Extract document type\n",
    "        doc_type = record.get('static_data', {}).get('summary', {}).get('doctypes', {}).get('doctype', [])\n",
    "\n",
    "        # Extract publisher information\n",
    "        publisher = ''\n",
    "        try:\n",
    "            publisher_info = record.get('static_data', {}).get('summary', {}).get('publishers', {}).get('publisher', {})\n",
    "            if publisher_info:\n",
    "                publisher = publisher_info.get('names', {}).get('name', {}).get('full_name', '')\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting publisher: {e}\")\n",
    "\n",
    "        # Extract publication year and date\n",
    "        pub_year = ''\n",
    "        pub_date = ''\n",
    "        try:\n",
    "            pub_info = record.get('static_data', {}).get('summary', {}).get('pub_info', {})\n",
    "            pub_year = pub_info.get('pubyear', '')\n",
    "            pub_date = pub_info.get('coverdate', '')\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting publication info: {e}\")\n",
    "\n",
    "        # Return extracted metadata\n",
    "        return {\n",
    "            'wos_id': wos_id,\n",
    "            'doi': doi,\n",
    "            'title': title,\n",
    "            'authors': authors,\n",
    "            'abstract': abstract,\n",
    "            'keywords': keywords,\n",
    "            'document_type': doc_type,\n",
    "            'publisher': publisher,\n",
    "            'publication_year': pub_year,\n",
    "            'publication_date': pub_date\n",
    "        }\n",
    "\n",
    "    except (KeyError, IndexError) as e:\n",
    "        # Handle specific key errors or index errors (missing or malformed data)\n",
    "        print(f\"Error extracting metadata: {e}\")\n",
    "        return {\n",
    "            'wos_id': '',\n",
    "            'doi': '',\n",
    "            'title': '',\n",
    "            'authors': [],\n",
    "            'abstract': '',\n",
    "            'keywords': [],\n",
    "            'document_type': [],\n",
    "            'publisher': '',\n",
    "            'publication_year': '',\n",
    "            'publication_date': ''\n",
    "        }\n",
    "    except Exception as e:\n",
    "        # Catch any other unexpected errors\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        return {\n",
    "            'wos_id': '',\n",
    "            'doi': '',\n",
    "            'title': '',\n",
    "            'authors': [],\n",
    "            'abstract': '',\n",
    "            'keywords': [],\n",
    "            'document_type': [],\n",
    "            'publisher': '',\n",
    "            'publication_year': '',\n",
    "            'publication_date': ''\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "def process_papers(papers, api_key, depth, visited_dois):\n",
    "    \"\"\"\n",
    "    Processes a list of papers to fetch their metadata and references up to a specified depth using a breadth-first approach.\n",
    "\n",
    "    Parameters:\n",
    "    papers (list): A list of paper dictionaries or DOIs (starting level).\n",
    "    api_key (str): The API key for authenticating with the Web of Science API.\n",
    "    depth (int): The maximum depth to fetch references.\n",
    "    visited_dois (set): A set to keep track of visited DOIs to avoid re-fetching.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of processed paper data, each containing metadata and references.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Queue to hold papers at each level (starting with initial papers)\n",
    "    queue = deque([(paper, 0) for paper in papers])  # (paper, current_depth)\n",
    "    processed_papers = []\n",
    "\n",
    "    current_depth = 0\n",
    "    while queue and current_depth <= depth:\n",
    "        # Filter out papers to process at the current depth\n",
    "        papers_at_current_depth = [p for p in queue if p[1] == current_depth]\n",
    "        if not papers_at_current_depth:\n",
    "            current_depth += 1\n",
    "            continue\n",
    "\n",
    "        # Use tqdm to show progress for the current depth\n",
    "        progress_bar = tqdm(papers_at_current_depth, desc=f\"Processing papers at depth {current_depth}\", leave=True)\n",
    "\n",
    "        for paper, _ in progress_bar:\n",
    "            queue.popleft()  # Remove the paper from the queue\n",
    "\n",
    "            # Extract DOI based on whether it's a dict or str\n",
    "            if isinstance(paper, dict):\n",
    "                paper_doi = paper.get('doi')\n",
    "            else:\n",
    "                paper_doi = paper\n",
    "\n",
    "            # If we haven't visited this DOI yet\n",
    "            if paper_doi and paper_doi not in visited_dois:\n",
    "                visited_dois.add(paper_doi)\n",
    "\n",
    "                # Fetch metadata and UID for the paper\n",
    "                metadata, uid = fetch_metadata_uid_using_doi(paper_doi, api_key)\n",
    "                if uid:\n",
    "                    filtered_metadata = extract_relevant_metadata(metadata)\n",
    "\n",
    "                    # If we are at max depth, don't fetch references\n",
    "                    if current_depth >= depth:\n",
    "                        references = []\n",
    "                    else:\n",
    "                        # Fetch references using UID\n",
    "                        references_data = fetch_references_using_uid(uid, api_key)\n",
    "                        \n",
    "                        # Extract DOIs of references and add them to the queue for the next level (depth + 1)\n",
    "                        # references = []\n",
    "                        if references_data:\n",
    "                            reference_dois = extract_dois(references_data)\n",
    "                            references = reference_dois\n",
    "                            for ref_doi in reference_dois:\n",
    "                                queue.append((ref_doi, current_depth + 1))  # Add references to queue for next depth level\n",
    "\n",
    "                    processed_papers.append({\n",
    "                        'doi': paper_doi,\n",
    "                        'metadata': filtered_metadata,\n",
    "                        'references': references\n",
    "                    })\n",
    "\n",
    "        # Close the progress bar for the current depth\n",
    "        progress_bar.close()\n",
    "\n",
    "        # Increment depth for the next batch\n",
    "        current_depth += 1\n",
    "\n",
    "    return processed_papers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_statistics(all_papers_data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Processes a collection of papers to compute and display statistics.\n",
    "    \n",
    "    Parameters:\n",
    "    - all_papers_data (list): A list of dictionaries where each dictionary represents a paper.\n",
    "      Each paper may contain metadata and references to other papers.\n",
    "    \n",
    "    Returns:\n",
    "    - None: This function prints the computed statistics directly.\n",
    "    \"\"\"\n",
    "    \n",
    "    total_papers = 0\n",
    "    total_references = 0\n",
    "    doc_type_counts = {'Article': 0, 'Book': 0, 'Other': 0}\n",
    "    depth_counts = {}\n",
    "\n",
    "    def traverse_papers(papers, current_depth):\n",
    "        \n",
    "        \"\"\"\n",
    "        Recursively traverses the papers and their references to update counts for \n",
    "        total papers, total references, document types, and depth levels.\n",
    "        \n",
    "        Parameters:\n",
    "        - papers (list): A list of dictionaries representing the papers at the current level.\n",
    "        - current_depth (int): The current depth in the reference hierarchy.\n",
    "        \n",
    "        Returns:\n",
    "        - None: Updates the statistics in place.\n",
    "        \"\"\"\n",
    "        \n",
    "        nonlocal total_papers, total_references\n",
    "        if current_depth not in depth_counts:\n",
    "            depth_counts[current_depth] = 0\n",
    "\n",
    "        for paper in papers:\n",
    "            total_papers += 1\n",
    "            depth_counts[current_depth] += 1\n",
    "\n",
    "            metadata = paper.get('metadata', {})\n",
    "            references = paper.get('references', [])\n",
    "            total_references += len(references)\n",
    "\n",
    "            doc_types = metadata.get('document_type', [])\n",
    "            if 'Article' in doc_types:\n",
    "                doc_type_counts['Article'] += 1\n",
    "            elif 'Book' in doc_types:\n",
    "                doc_type_counts['Book'] += 1\n",
    "            else:\n",
    "                doc_type_counts['Other'] += 1\n",
    "\n",
    "            traverse_papers(references, current_depth + 1)\n",
    "\n",
    "    traverse_papers(all_papers_data, current_depth=0)\n",
    "\n",
    "\n",
    "    print(\"\\nStatistics:\")\n",
    "    print(f\"Total papers processed: {total_papers}\")\n",
    "    \n",
    "    for depth, count in depth_counts.items():\n",
    "        print(f\"Number of papers processed at depth {depth}: {count}\")\n",
    "        \n",
    "    print(f\"Number of articles: {doc_type_counts['Article']}\")\n",
    "    print(f\"Number of books: {doc_type_counts['Book']}\")\n",
    "    print(f\"Documents with Other document types: {doc_type_counts['Other']}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def main(visited_dois=set()):\n",
    "\n",
    "    \"\"\"\n",
    "    Main function to load seed papers from a BibTeX file, process them to fetch metadata and references,\n",
    "    and save the results to a JSON file.\n",
    "    \"\"\"\n",
    "\n",
    "    with open('seedPapers.bib') as bibtex_file:\n",
    "        bib_database = bibtexparser.load(bibtex_file)\n",
    "    \n",
    "    papers = bib_database.entries\n",
    "\n",
    "    api_key = 'your-api-key'\n",
    "    depth = 3\n",
    "    \n",
    "\n",
    "    folder_path = 'social_unrest_metadata_depth3_bfs'\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    for i in range(len(papers)): # len(papers)\n",
    "        seed_paper_data = process_papers(papers[i:i+1], api_key, depth, visited_dois) ######\n",
    "        file_name = os.path.join(folder_path, f\"seed_paper_{i+1}.json\")\n",
    "        with open(file_name, 'w') as f:\n",
    "            json.dump(seed_paper_data, f, indent=4)\n",
    "        print(f\"Seed Paper : {i+1}/16\")\n",
    "        display_statistics(seed_paper_data)\n",
    "        \n",
    "    return visited_dois\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    visited_dois_set = main()\n",
    "    # print(len(visited_dois_set))\n",
    "    # visited_dois_set = main(visited_dois_set)\n",
    "    # print(len(visited_dois_set))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
